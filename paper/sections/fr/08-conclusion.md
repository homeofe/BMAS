# üéØ 8. Conclusion

Ce travail a introduit **Blind Multi-Agent Synthesis (BMAS)**, une m√©thodologie pour √©liciter, comparer et synth√©tiser les r√©ponses de plusieurs grands mod√®les de langage dans une isolation stricte, et a pr√©sent√© des r√©sultats empiriques d'une exp√©rience de 540 lancements sur douze LLMs frontier et trois strates de domaines.

## üìã 8.1 R√©sum√© des contributions

Nous avons d√©montr√© que :

1. **La convergence est d√©pendante du domaine et mesurable.** Sur 45 prompts, les domaines A et B (technique et r√©glementaire) ont montr√© une similarit√© s√©mantique inter-mod√®les syst√©matiquement plus √©lev√©e que le domaine C (prompts strat√©giques et ambigus). [Voir la section 4 pour les valeurs exactes.]

2. **La divergence signale l'erreur dans les domaines factuels.** Les mod√®les identifi√©s comme valeurs aberrantes s√©mantiques par le clustering DBSCAN ont montr√© une pr√©cision factuelle plus faible par rapport aux r√©ponses de r√©f√©rence pr√©-enregistr√©es que les mod√®les non-aberrants, soutenant l'hypoth√®se H2. Cela fournit une base empirique pour l'utilisation de la divergence comme portail de qualit√© pratique dans les syst√®mes de d√©cision assist√©s par IA.

3. **La qualit√© de la synth√®se varie selon la strat√©gie et le domaine.** La synth√®se LLM-as-Judge (S3) a produit la pr√©cision factuelle la plus √©lev√©e sur les domaines A et B, tandis que le vote majoritaire (S1) a fourni la couverture la plus compl√®te. Le centro√Øde s√©mantique (S2) a mieux fonctionn√© comme r√©sum√© repr√©sentatif concis. Aucune strat√©gie unique n'a domin√© tous les types de prompts.

4. **La longueur des tokens n'est pas un proxy de qualit√©.** Nous avons observ√© une variation significative du nombre de tokens de r√©ponse entre mod√®les sur des prompts identiques (jusqu'√† un ratio de 6,5 pour certains prompts), sans corr√©lation coh√©rente entre la longueur de r√©ponse et la pr√©cision factuelle. Gemini 2.5-pro √©tait syst√©matiquement le plus verbeux ; Sonar le plus concis. Ces diff√©rences stylistiques ne pr√©disent pas la convergence.

## 8.2 Enseignements pratiques

Pour les praticiens d√©ployant des LLMs dans des environnements r√©glement√©s ou √† enjeux √©lev√©s, BMAS sugg√®re une architecture pratique : ex√©cuter les prompts sur plusieurs fournisseurs de mod√®les ind√©pendants, mesurer la convergence s√©mantique et router les r√©ponses √† faible confiance (forte divergence) vers une revue humaine. La charge est justifi√©e par le gain de fiabilit√©, particuli√®rement pour les questions critiques de conformit√© o√π une seule mauvaise r√©ponse a des cons√©quences juridiques ou de s√©curit√©.

Le protocole de pr√©-enregistrement utilis√© dans cette √©tude - verrouiller les r√©ponses de r√©f√©rence avant tout lancement de mod√®le - est transf√©rable √† tout effort d'√©valuation multi-mod√®les et pr√©vient les biais de confirmation qui peuvent survenir lorsque les √©valuateurs connaissent les r√©ponses avant de concevoir les m√©triques.

## 8.3 Relation avec AAHP et failprompt

BMAS a √©t√© d√©velopp√© dans le contexte d'AAHP (AI-to-AI Handoff Protocol), un cadre structur√© d'orchestration multi-agents pour les pipelines d'IA en production, et de failprompt, un outil CLI pour valider les r√©ponses IA dans les environnements CI/CD. Ensemble, ces trois projets forment un toolkit int√©gr√© pour le d√©ploiement responsable d'IA multi-mod√®les : AAHP fournit la couche d'orchestration, failprompt fournit le portail CI, et BMAS fournit la base empirique pour comprendre quand et pourquoi le consensus multi-mod√®les est plus fiable que la sortie d'un mod√®le unique.

Tous les codes, prompts, r√©ponses de r√©f√©rence pr√©-enregistr√©es et r√©sultats exp√©rimentaux sont publi√©s en tant que jeux de donn√©es ouverts pour soutenir la r√©plication et l'extension de ce travail.

---

*Le jeu de donn√©es BMAS, le runner, le pipeline de m√©triques et le code de synth√®se sont disponibles √† l'adresse : https://github.com/homeofe/BMAS*
