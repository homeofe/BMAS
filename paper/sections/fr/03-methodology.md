# üî¨ 3. M√©thodologie

## 3.1 Vue d'ensemble du protocole BMAS

Blind Multi-Agent Synthesis (BMAS) est un protocole en quatre phases pour √©liciter, comparer et synth√©tiser les r√©ponses de plusieurs LLMs sur des prompts identiques. Les quatre phases sont :

1. **Elicitation aveugle** - Chaque mod√®le re√ßoit le m√™me prompt sans connaissance de l'√©tude, des autres mod√®les ou des autres r√©ponses.
2. **Calcul des m√©triques** - La similarit√© s√©mantique par paires, la pr√©cision factuelle et la d√©tection des valeurs aberrantes sont calcul√©es pour toutes les r√©ponses des mod√®les.
3. **Synth√®se** - Trois strat√©gies de synth√®se agr√®gent les r√©ponses individuelles en une seule sortie.
4. **Evaluation** - Les sorties de synth√®se sont √©valu√©es par rapport aux r√©ponses de r√©f√©rence pr√©-enregistr√©es (domaines A et B) ou par l'√©valuation d'experts (domaine C).

Le protocole impose une stricte **r√®gle de non-contamination** : aucune r√©ponse de mod√®le n'est rendue disponible √† aucun autre mod√®le √† aucune phase avant la synth√®se. Cela refl√®te l'exigence d'isolation de la m√©thode Delphi et distingue BMAS des approches multi-agents coop√©ratives telles que MoA (Wang et al., 2024).

## 3.2 Mod√®les

Nous √©valuons douze LLMs de pointe de quatre fournisseurs distincts :

| ID | Mod√®le | Fournisseur | Fen√™tre de contexte |
|---|---|---|---|
| M1 | claude-sonnet-4-6 | Anthropic | 1M tokens |
| M2 | claude-opus-4-6 | Anthropic | 1M tokens |
| M3 | gpt-5.3-codex | OpenAI | 272k tokens |
| M4 | gemini-2.5-pro | Google | 1M tokens |
| M5 | sonar-pro | Perplexity | 127k tokens |

La diversit√© multi-fournisseurs est d√©lib√©r√©e. Les mod√®les du m√™me fournisseur partagent une lign√©e architecturale et des pipelines de donn√©es d'entra√Ænement, ce qui peut r√©duire la divergence m√™me dans des conditions aveugles. L'inclusion de mod√®les de quatre fournisseurs s√©par√©s maximise l'ind√©pendance des r√©ponses.

**Impl√©mentation de l'isolation :** Chaque mod√®le fonctionne dans une session isol√©e s√©par√©e sans contexte partag√©. Le prompt syst√®me est identique pour tous les mod√®les :

> *"Vous √™tes un assistant expert comp√©tent. R√©pondez √† la question suivante de mani√®re aussi pr√©cise et compl√®te que possible. Soyez pr√©cis, factuel et structur√©. Si vous n'√™tes pas certain d'un d√©tail sp√©cifique, indiquez-le explicitement."*

La temp√©rature n'est pas modifi√©e. Nous pr√©servons d√©lib√©r√©ment le comportement d'√©chantillonnage par d√©faut de chaque mod√®le pour capturer la variance naturelle des r√©ponses, non pour la normaliser.

## 3.3 Conception des prompts

### 3.3.1 Structure des domaines

Nous avons construit 45 prompts sur trois strates de domaines :

**Domaine A - Questions techniques de haute pr√©cision (A01-A10) :** Questions avec des r√©ponses objectivement correctes v√©rifiables par rapport √† des sources primaires faisant autorit√© (normes NIST FIPS, NVD, RFCs IETF, sp√©cifications OpenID Foundation). Exemples : justification du scoring CVSS, tailles de cl√©s des algorithmes PQC, √©num√©ration des suites de chiffrement TLS 1.3.

**Domaine B - R√©glementaire/Conformit√© (B01-B10) :** Questions fond√©es sur des textes juridiques et r√©glementaires avec des sources faisant autorit√© (RGPD, eIDAS 2.0, NIS2, ISO 27001, BSI C5). Un certain jugement interpr√©tatif est requis aux marges, mais les r√©ponses centrales sont d√©finies dans des textes formels. Exemples : exceptions d'effacement de l'Article 17(3) du RGPD, classifications sectorielles NIS2, diff√©rences de niveaux d'√©valuation TISAX.

**Domaine C - Strat√©gique/Ambigu (C01-C10) :** Questions sans r√©ponse unique correcte, n√©cessitant un jugement d'expert et un raisonnement architectural. Plusieurs positions d√©fendables existent. Exemples : d√©cisions d'architecture zero-trust, priorisation de la migration PQC, arbitrages d'investissements de conformit√©.

### 3.3.2 Exigences des prompts

Tous les prompts ont √©t√© con√ßus pour satisfaire quatre crit√®res :

1. **Autonome** - r√©pondable sans contexte externe ni r√©cup√©ration de documents
2. **R√©ponse structur√©e** - chaque prompt sp√©cifie un format de sortie requis (liste, comparaison, d√©cision avec justification)
3. **Longueur born√©e** - r√©ponse attendue de 300-600 tokens pour les domaines A-B ; 400-800 pour le domaine C
4. **Testable** - pour les domaines A et B, une r√©ponse v√©rifiable existe ; pour le domaine C, l'√©valuation par expert est pratiquement r√©alisable

### 3.3.3 Pr√©-enregistrement

Suivant les bonnes pratiques de la science ouverte, les r√©ponses de r√©f√©rence pour les domaines A et B ont √©t√© document√©es et verrouill√©es avant tout lancement de mod√®le. Cela pr√©vient les biais de confirmation inconscients dans le scoring. Les documents de r√©ponses de r√©f√©rence sont publi√©s avec le jeu de donn√©es.

Le domaine C n'a pas de r√©ponses de r√©f√©rence pr√©-enregistr√©es. L'√©valuation par expert (l'auteur principal comme expert du domaine) √©value la qualit√© de la synth√®se selon une rubrique de compl√©tude, de qualit√© du raisonnement et d'exploitabilit√© pratique.

## 3.4 M√©triques

### 3.4.1 Similarit√© s√©mantique (primaire)

Nous calculons la similarit√© cosinus par paires entre les embeddings de r√©ponses en utilisant le mod√®le sentence-transformer `all-mpnet-base-v2` (Reimers et Gurevych, 2019). Pour N mod√®les, cela produit une matrice de similarit√© N x N par prompt. Nous rapportons :

- **Similarit√© par paires moyenne (MPS) :** moyenne de tous les scores par paires N(N-1)/2
- **Similarit√© minimale par paires :** la paire la plus divergente
- **Ecart-type de similarit√© :** variance au sein du cluster de r√©ponses du prompt

### 3.4.2 BERTScore

Nous calculons BERTScore F1 par paires (Zhang et al., 2020) comme mesure secondaire de similarit√© s√©mantique au niveau des tokens. BERTScore capture la proximit√© lexicale au-del√† des embeddings au niveau des phrases et est sensible au chevauchement des affirmations factuelles.

### 3.4.3 Jaccard sur les affirmations cl√©s

Nous extrayons des affirmations factuelles discr√®tes de chaque r√©ponse en utilisant la segmentation en phrases et calculons la similarit√© Jaccard par paires sur des ensembles d'affirmations normalis√©es. Cette m√©trique capture l'accord structurel - si les mod√®les identifient les m√™mes points cl√©s - ind√©pendamment de la formulation.

### 3.4.4 D√©tection des valeurs aberrantes

Nous appliquons DBSCAN (Ester et al., 1996) √† l'espace d'embedding avec eps=0.15 (√©quivalent √† une similarit√© cosinus inf√©rieure √† 0.85) et min_samples=2. Les mod√®les dont les embeddings tombent en dehors de tous les clusters de voisinage re√ßoivent une √©tiquette aberrante (-1). Nous traitons le statut d'aberrant comme un signal de hallucination potentielle dans les domaines A et B, et comme indicateur de point de vue minoritaire dans le domaine C.

### 3.4.5 Pr√©cision factuelle (domaines A et B uniquement)

Pour chaque r√©ponse des domaines A et B, nous √©valuons la pr√©cision factuelle par rapport √† la liste de contr√¥le des r√©ponses de r√©f√©rence pr√©-enregistr√©es. Chaque item de la liste de contr√¥le est binaire (pr√©sent et correct, ou absent/incorrect). Le score de pr√©cision factuelle est la fraction d'items de la liste de contr√¥le satisfaits.

## 3.5 Strat√©gies de synth√®se

Nous √©valuons trois strat√©gies de synth√®se :

**S1 - Vote majoritaire (niveau des affirmations) :** Les affirmations factuelles sont extraites de toutes les r√©ponses. Une affirmation est accept√©e dans la synth√®se si elle appara√Æt dans les r√©ponses d'au moins 58 % des mod√®les (sept sur douze). Les affirmations minoritaires (sous le seuil) sont ajout√©es avec un marqueur [MINORITY].

**S2 - Centro√Øde s√©mantique :** La r√©ponse dont l'embedding est le plus proche de la moyenne de tous les embeddings de r√©ponses est s√©lectionn√©e comme base de synth√®se. Cela capture la r√©ponse individuelle la plus "repr√©sentative". Aucun nouveau contenu n'est ajout√©.

**S3 - LLM-as-Judge :** Les douze r√©ponses anonymis√©es sont pr√©sent√©es √† une treizi√®me instance de mod√®le (M2, claude-opus-4-6) avec pour instruction de produire une synth√®se faisant autorit√© unique, en marquant les affirmations minoritaires ([MINORITY]) et les contradictions ([DISPUTED]). Le mod√®le juge ne re√ßoit aucune information identifiant quel mod√®le a produit quelle r√©ponse.

La qualit√© de la synth√®se est √©valu√©e en mesurant la pr√©cision factuelle du texte r√©sultant par rapport aux r√©ponses de r√©f√©rence pr√©-enregistr√©es (domaines A et B) et par scoring selon une rubrique d'expert (domaine C).

## 3.6 Hypoth√®ses

Nous testons trois hypoth√®ses pr√©-enregistr√©es :

**H1 (Convergence dans les domaines factuels) :** La similarit√© s√©mantique par paires moyenne pour les prompts des domaines A et B d√©passera 0.75 (BERTScore F1).

**H2 (La divergence signale l'erreur) :** Parmi les r√©ponses des domaines A et B, les mod√®les aberrants (label DBSCAN -1) auront des scores de pr√©cision factuelle significativement plus faibles que les mod√®les non-aberrants (test t unilat√©ral, alpha=0.05).

**H3 (Effet du domaine sur la convergence) :** La similarit√© par paires moyenne pour le domaine C sera significativement plus faible que pour les domaines A et B (ANOVA √† sens unique, Tukey HSD post-hoc, alpha=0.05).

## 3.7 Configuration exp√©rimentale

Tous les lancements de mod√®les ont √©t√© ex√©cut√©s via la passerelle OpenClaw, qui route ind√©pendamment vers l'API de chaque fournisseur. Chaque prompt s'ex√©cute dans une session isol√©e s√©par√©e sans √©tat partag√©. Les sorties des lancements sont stock√©es en JSON structur√© avec l'ID du prompt, l'ID du mod√®le, le texte de la r√©ponse, le nombre de tokens et la latence. Le jeu de donn√©es complet de 540 lancements (45 prompts x 5 mod√®les) est publi√© avec ce travail.
