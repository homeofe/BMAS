# üìö 2. Trabajos relacionados

BMAS se basa en m√©todos de consenso experto estructurado, t√©cnicas LLM multi-muestra y multi-modelo, m√©tricas de evaluaci√≥n automatizada y clustering basado en densidad. Esta secci√≥n revisa cada √°rea y clarifica el posicionamiento de BMAS respecto a trabajos previos.

## 2.1 El m√©todo Delphi

Dalkey y Helmer (1963) introdujeron el m√©todo Delphi en la RAND Corporation como enfoque estructurado de predicci√≥n experta. En el protocolo original, un panel de expertos proporcionaba estimaciones independientes sin conocer las respuestas de los dem√°s, y un facilitador agregaba los resultados en m√∫ltiples rondas iterativas. La fortaleza central del m√©todo era que el aislamiento preven√≠a el anclaje y el pensamiento grupal, permitiendo que el desacuerdo genuino emergiera antes de buscar el consenso. BMAS toma prestado este principio de aislamiento directamente: cada LLM responde a los prompts sin observar las salidas de ning√∫n otro modelo, garantizando que la convergencia, cuando ocurre, refleja razonamiento independiente y no imitaci√≥n.

## 2.2 Self-Consistency

Wang et al. (2022) propusieron la self-consistency como estrategia de decodificaci√≥n que muestrea m√∫ltiples cadenas de razonamiento de un √∫nico modelo de lenguaje y selecciona la respuesta final por voto mayoritario. El m√©todo demostr√≥ mejoras significativas en benchmarks de razonamiento aritm√©tico y de sentido com√∫n al explotar la intuici√≥n de que los caminos de razonamiento correctos tienen m√°s probabilidad de converger en la misma respuesta que los incorrectos. Sin embargo, dado que todas las cadenas de razonamiento provienen del mismo modelo, la self-consistency captura solo la varianza de decodificaci√≥n intra-modelo, no las diferencias m√°s profundas en datos de entrenamiento, arquitectura y alineaci√≥n que distinguen a diferentes proveedores. BMAS extiende la intuici√≥n de convergencia-como-se√±al-de-calidad al marco multi-proveedor, donde el acuerdo entre modelos entrenados independientemente constituye un prior m√°s fuerte para la correcci√≥n.

## 2.3 Mixture of Agents

Wang et al. (2024) introdujeron el framework Mixture-of-Agents (MoA), en el que varios LLMs participan en rondas de agregaci√≥n iterativas donde cada modelo puede observar y refinar los outputs de los dem√°s. MoA demostr√≥ que el refinamiento colaborativo entre modelos mejoraba el rendimiento en benchmarks como AlpacaEval y MT-Bench. La diferencia cr√≠tica con BMAS es que MoA no es ciego: los modelos en rondas posteriores est√°n expuestos a outputs previos, lo que introduce el riesgo de propagaci√≥n de errores. BMAS evita deliberadamente esto imponiendo un aislamiento estricto durante la fase de respuesta y difiriendo cualquier interacci√≥n entre modelos a una fase de s√≠ntesis separada.

## 2.4 LLM-as-Judge

Zheng et al. (2023) investigaron el uso de grandes modelos de lenguaje como evaluadores de outputs de otros modelos, introduciendo los benchmarks MT-Bench y Chatbot Arena. Su trabajo mostr√≥ que los LLMs potentes pod√≠an servir como proxies escalables para la evaluaci√≥n humana. En BMAS, el papel de juez se limita a una de las tres estrategias de s√≠ntesis (S3): un decimotercer modelo sintetiza las doce respuestas ciegas en un √∫nico output, pero la correcci√≥n se mide en √∫ltima instancia frente a respuestas de referencia pre-registradas, no frente a las preferencias del juez.

## 2.5 BERTScore

Zhang et al. (2020) propusieron BERTScore, una m√©trica de evaluaci√≥n autom√°tica que calcula la similitud a nivel de tokens entre textos candidato y de referencia usando embeddings contextuales de modelos transformer pre-entrenados. A diferencia de las m√©tricas de solapamiento de n-gramas como BLEU o ROUGE, BERTScore captura la equivalencia sem√°ntica a trav√©s de diferentes formas superficiales y es robusta a la par√°frasis. BMAS adopta BERTScore F1 como m√©trica principal de similitud por pares para medir la convergencia inter-modelos.

## 2.6 Constitutional AI

Bai et al. (2022) introdujeron Constitutional AI (CAI) en Anthropic, una metodolog√≠a de entrenamiento en la que un modelo critica y revisa sus propios outputs seg√∫n un conjunto de principios antes del aprendizaje por refuerzo con retroalimentaci√≥n humana. BMAS puede verse como la extensi√≥n de la intuici√≥n de cr√≠tica-y-revisi√≥n de un bucle de modelo √∫nico a un marco multi-modelo y multi-proveedor: en lugar de un modelo que se juzga a s√≠ mismo, varios modelos entrenados independientemente sirven como cr√≠ticos impl√≠citos entre s√≠ a trav√©s de la se√±al de divergencia.

## 2.7 DBSCAN

Ester et al. (1996) propusieron DBSCAN (Density-Based Spatial Clustering of Applications with Noise), un algoritmo de clustering que agrupa puntos de datos bas√°ndose en la conectividad de densidad e identifica los puntos en regiones de baja densidad como ruido o valores at√≠picos. A diferencia de k-means, DBSCAN no requiere especificar el n√∫mero de clusters a priori. BMAS emplea DBSCAN sobre el espacio de embeddings de las respuestas de los modelos para detectar outputs at√≠picos: en dominios factuales, una respuesta at√≠pica se trata como una alucinaci√≥n candidata.

## 2.8 Posicionamiento

BMAS es, seg√∫n nuestro conocimiento, el primer framework que combina cuatro propiedades ausentes de cualquier enfoque previo individual. Primero, impone aislamiento ciego entre proveedores. Segundo, introduce an√°lisis estratificado por dominio. Tercero, trata la divergencia como se√±al de anomal√≠a en lugar de fallo de coordinaci√≥n. Cuarto, proporciona una comparaci√≥n controlada de estrategias de s√≠ntesis evaluadas frente a respuestas de referencia pre-registradas.
