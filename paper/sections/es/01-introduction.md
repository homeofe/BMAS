# 游닇 1. Introducci칩n

Los modelos de lenguaje grande han alcanzado un nivel de capacidad suficiente para ser desplegados en dominios donde la precisi칩n no es opcional: an치lisis jur칤dico, diagn칩stico m칠dico, cumplimiento regulatorio y sistemas de identidad gubernamentales. En estos dominios, una respuesta segura pero incorrecta de un modelo 칰nico no es un inconveniente menor - es un fallo con consecuencias reales.

El enfoque dominante para mejorar la fiabilidad de los LLM es o bien un mejor entrenamiento (RLHF, Constitutional AI) o bien un mejor prompting (cadena de pensamiento, aumento por recuperaci칩n). Ambos operan dentro de un paradigma de modelo 칰nico: un modelo, una salida, una respuesta en la que confiar o no.

Este trabajo adopta un enfoque diferente. En lugar de preguntar "c칩mo hacemos un modelo m치s fiable", preguntamos: **쯤u칠 podemos aprender del desacuerdo entre m칰ltiples modelos que no pueden influirse mutuamente?**

## 1.1 La intuici칩n central

Cuando cinco expertos independientes responden a la misma pregunta sin consultarse entre s칤, y cuatro de ellos dan la misma respuesta mientras uno da una diferente, no concluimos que los cuatro est치n equivocados. Examinamos la respuesta disidente con m치s cuidado, pero confiamos en el consenso como punto de partida.

Este es el m칠todo Delphi, aplicado desde 1963 a la predicci칩n experta. Su fortaleza es estructural: **el aislamiento previene el pensamiento grupal; el consenso emerge del razonamiento independiente, no de la presi칩n social.**

BMAS aplica esta l칩gica a los LLMs. Cada modelo es un experto con una distribuci칩n de entrenamiento particular, un horizonte de conocimiento y un conjunto de sesgos. Cuando se les a칤sla entre s칤 y se les hace la misma pregunta, su convergencia o divergencia es en s칤 misma informativa.

## 1.2 Qu칠 hay de nuevo

Varios trabajos previos son relacionados pero distintos:

**Self-Consistency** (Wang et al., 2022) genera m칰ltiples cadenas de razonamiento a partir de un *칰nico* modelo y usa votaci칩n mayoritaria. BMAS utiliza modelos *diferentes* - esto prueba a trav칠s de distribuciones de entrenamiento, no solo varianza de decodificaci칩n.

**Mixture of Agents** (Wang et al., 2024) permite a los modelos ver las salidas de los dem치s en rondas de agregaci칩n. Esto produce refinamiento colaborativo, pero introduce el riesgo de propagaci칩n de errores: si un modelo produce una alucinaci칩n segura en la primera ronda, los modelos siguientes pueden anclarse a ella.

**LLM-as-Judge** (Zheng et al., 2023) usa un modelo para evaluar a otro. BMAS usa un modelo para *sintetizar* las salidas de varios otros - el papel de juez se limita a la fase final de s칤ntesis.

BMAS es el primer framework que combina cuatro propiedades:
1. Aislamiento ciego estricto (sin contaminaci칩n cruzada)
2. Diversidad de modelos (distintos proveedores, arquitecturas, distribuciones de entrenamiento)
3. An치lisis estratificado por dominio (factual, regulatorio, estrat칠gico)
4. Divergencia como se침al (no como fallo)

## 1.3 Motivaci칩n pr치ctica

Esta investigaci칩n surgi칩 de la experiencia operativa construyendo AEGIS, un sistema de verificaci칩n de identidad gubernamental transfronterizo de la UE, y AAHP (AI-to-AI Handoff Protocol), un framework estructurado de orquestaci칩n multi-agente. En ambos sistemas, los pipelines multi-agente se utilizan para decisiones de arquitectura, an치lisis de cumplimiento y revisi칩n de implementaciones.

Surgi칩 una pregunta pr치ctica: cuando se utilizan m칰ltiples LLMs como revisores independientes en un pipeline, 쯖u치nto difieren realmente sus salidas? 쯏 cuando difieren, qui칠n tiene raz칩n?

BMAS es la respuesta formal a esa pregunta.

## 1.4 Contribuciones

Este trabajo aporta:

1. **Metodolog칤a BMAS:** Un protocolo formalizado de s칤ntesis ciega multi-agente con restricciones de aislamiento, conjunto de m칠tricas y estrategias de s칤ntesis.
2. **Estudio emp칤rico:** Resultados de 45 prompts para 12 LLMs en 3 estratos de dominio, con respuestas de referencia pre-registradas para los dominios A y B.
3. **Validaci칩n de la hip칩tesis divergencia-como-se침al:** Evidencia estad칤stica de que la divergencia entre modelos predice la tasa de errores factuales.
4. **Comparaci칩n de estrategias de s칤ntesis:** Evaluaci칩n emp칤rica del voto mayoritario, centroide sem치ntico y s칤ntesis LLM-as-Judge frente a respuestas de referencia.
5. **Dataset abierto:** Todos los prompts, respuestas crudas de los modelos e indicadores de m칠tricas publicados como benchmark p칰blico.

## 1.5 Estructura del art칤culo

La secci칩n 2 revisa trabajos relacionados. La secci칩n 3 describe la metodolog칤a BMAS y el dise침o experimental. La secci칩n 4 presenta los resultados. La secci칩n 5 analiza correlaciones divergencia-alucinaci칩n. La secci칩n 6 eval칰a las estrategias de s칤ntesis. La secci칩n 7 discute implicaciones, limitaciones y trabajos futuros. La secci칩n 8 concluye.
