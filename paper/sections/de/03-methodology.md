# üî¨ 3. Methodik

## 3.1 √úberblick √ºber das BMAS-Protokoll

Blind Multi-Agent Synthesis (BMAS) ist ein vierphasiges Protokoll zur Erhebung, zum Vergleich und zur Synthese von Antworten mehrerer LLMs auf identische Prompts. Die vier Phasen sind:

1. **Blinde Erhebung** - Jedes Modell erh√§lt denselben Prompt ohne Kenntnis der Studie, anderer Modelle oder anderer Antworten.
2. **Metrikberechnung** - Paarweise semantische √Ñhnlichkeit, faktische Genauigkeit und Ausrei√üererkennung werden √ºber alle Modellantworten berechnet.
3. **Synthese** - Drei Synthesestrategien aggregieren die Einzelantworten zu einer einzigen Ausgabe.
4. **Evaluation** - Syntheseausgaben werden anhand vorregistrierter Korrektantworten (Dom√§nen A und B) oder Expertenevaluation (Dom√§ne C) bewertet.

Das Protokoll erzwingt eine strikte **Kontaminationsfreiheitsregel**: Keine Modellantwort wird einem anderen Modell in irgendeiner Phase vor der Synthese zug√§nglich gemacht. Dies entspricht der Isolationsanforderung der Delphi-Methode und unterscheidet BMAS von kooperativen Multi-Agenten-Ans√§tzen wie MoA (Wang et al., 2024).

## 3.2 Modelle

Wir evaluieren zw√∂lf modernste LLMs von vier verschiedenen Anbietern:

| ID | Modell | Anbieter | Kontextfenster |
|---|---|---|---|
| M1 | claude-sonnet-4-6 | Anthropic | 1M Tokens |
| M2 | claude-opus-4-6 | Anthropic | 1M Tokens |
| M3 | gpt-5.3-codex | OpenAI | 272k Tokens |
| M4 | gemini-2.5-pro | Google | 1M Tokens |
| M5 | sonar-pro | Perplexity | 127k Tokens |

Die anbieter√ºbergreifende Diversit√§t ist bewusst gew√§hlt. Modelle desselben Anbieters teilen architektonische Herkunft und Trainingsdaten-Pipelines, was die Divergenz auch unter Blindbedingungen reduzieren kann. Die Einbeziehung von Modellen von vier verschiedenen Anbietern maximiert die Unabh√§ngigkeit der Antworten.

**Isolations-Implementierung:** Jedes Modell l√§uft in einer separaten isolierten Session ohne gemeinsamen Kontext. Der System-Prompt ist √ºber alle Modelle hinweg identisch:

> *"Sie sind ein sachkundiger Expertenassistent. Beantworten Sie die folgende Frage so genau und vollst√§ndig wie m√∂glich. Seien Sie pr√§zise, sachlich und strukturiert. Wenn Sie √ºber ein bestimmtes Detail unsicher sind, geben Sie das ausdr√ºcklich an."*

Die Temperatur wird nicht √ºberschrieben. Wir bewahren das Standard-Sampling-Verhalten jedes Modells, um nat√ºrliche Antwortvarianz zu erfassen, nicht um sie zu normalisieren.

## 3.3 Prompt-Design

### 3.3.1 Dom√§nenstruktur

Wir haben 45 Prompts √ºber drei Dom√§nenstrata konstruiert:

**Dom√§ne A - Hochpr√§zise Technische Fragen (A01-A10):** Fragen mit objektiv richtigen Antworten, die gegen prim√§re autoritative Quellen verifizierbar sind (NIST-FIPS-Standards, NVD, IETF-RFCs, OpenID-Foundation-Spezifikationen). Beispiele: CVSS-Scoring-Begr√ºndung, PQC-Algorithmen-Schl√ºsselgr√∂√üen, TLS-1.3-Cipher-Suite-Enumeration.

**Dom√§ne B - Regulatorisch/Compliance (B01-B10):** Fragen, die auf rechtlichem und regulatorischem Text mit autoritativen Quellen basieren (DSGVO, eIDAS 2.0, NIS2, ISO 27001, BSI C5). An den R√§ndern ist ein gewisses interpretatives Urteil erforderlich, aber die Kernantworten sind in formalem Text definiert. Beispiele: DSGVO-Artikel-17(3)-L√∂schungsausnahmen, NIS2-Sektoreinteilungen, TISAX-Assessment-Level-Unterschiede.

**Dom√§ne C - Strategisch/Mehrdeutig (C01-C10):** Fragen ohne einzeln korrekte Antwort, die Expertenurteil und architekturelles Denken erfordern. Mehrere vertretbare Positionen existieren. Beispiele: Zero-Trust-Architekturentscheidungen, PQC-Migrationspriorisierung, Compliance-Investitionsabw√§gungen.

### 3.3.2 Prompt-Anforderungen

Alle Prompts wurden so gestaltet, dass sie vier Kriterien erf√ºllen:

1. **Eigenst√§ndig** - ohne externe Kontexte oder Dokumentabruf beantwortbar
2. **Strukturierte Antwort** - jeder Prompt gibt ein erforderliches Ausgabeformat vor (Liste, Vergleich, Entscheidung mit Begr√ºndung)
3. **Begrenzte L√§nge** - erwartete Antwort von 300-600 Tokens f√ºr Dom√§nen A-B; 400-800 f√ºr Dom√§ne C
4. **√úberpr√ºfbar** - f√ºr Dom√§nen A und B existiert eine verifizierbare Antwort; f√ºr Dom√§ne C ist Expertenevaluation praktisch durchf√ºhrbar

### 3.3.3 Vorregistrierung

Gem√§√ü Open-Science-Best-Practices wurden Korrektantworten f√ºr Dom√§nen A und B vor jeglichen Modelll√§ufen dokumentiert und gesperrt. Dies verhindert unbewusste Best√§tigungsverzerrung beim Scoring. Korrektantwortdokumente werden zusammen mit dem Datensatz ver√∂ffentlicht.

Dom√§ne C hat keine vorregistrierten Korrektantworten. Expertenevaluation (der Erstautor als Dom√§nenexperte) bewertet die Synthesequalit√§t anhand einer Rubrik aus Vollst√§ndigkeit, Reasoning-Qualit√§t und praktischer Handlungsf√§higkeit.

## 3.4 Metriken

### 3.4.1 Semantische √Ñhnlichkeit (Prim√§r)

Wir berechnen paarweise Kosinus-√Ñhnlichkeit zwischen Antwort-Embeddings unter Verwendung des `all-mpnet-base-v2`-Sentence-Transformer-Modells (Reimers und Gurevych, 2019). F√ºr N Modelle ergibt dies eine N-x-N-√Ñhnlichkeitsmatrix pro Prompt. Wir berichten:

- **Mittlere paarweise √Ñhnlichkeit (MPS):** Durchschnitt aller N(N-1)/2 paarweisen Scores
- **Minimale paarweise √Ñhnlichkeit:** das divergenteste Paar
- **Standardabweichung der √Ñhnlichkeit:** Varianz innerhalb des Antwortclusters eines Prompts

### 3.4.2 BERTScore

Wir berechnen paarweisen BERTScore F1 (Zhang et al., 2020) als sekund√§res Token-Level-Semantik-√Ñhnlichkeitsma√ü. BERTScore erfasst lexikalische N√§he jenseits von Satz-Level-Embeddings und ist sensitiv f√ºr faktische Behauptungs√ºberlappung.

### 3.4.3 Jaccard auf Schl√ºsselbehauptungen

Wir extrahieren diskrete faktische Behauptungen aus jeder Antwort mittels Satzsegmentierung und berechnen paarweise Jaccard-√Ñhnlichkeit auf normalisierten Behauptungsmengen. Diese Metrik erfasst strukturelle √úbereinstimmung - ob Modelle dieselben Kernpunkte identifizieren - unabh√§ngig von der Formulierung.

### 3.4.4 Ausrei√üererkennung

Wir wenden DBSCAN (Ester et al., 1996) auf den Embedding-Raum mit eps=0.15 (entspricht Kosinus-√Ñhnlichkeit < 0.85) und min_samples=2 an. Modelle, deren Embeddings au√üerhalb aller Nachbarschaftscluster fallen, erhalten ein Ausrei√üerlabel (-1). Wir behandeln Ausrei√üerstatus in Dom√§nen A und B als Signal f√ºr potenzielle Halluzination und in Dom√§ne C als Minderheitsansichtsindikator.

### 3.4.5 Faktische Genauigkeit (nur Dom√§nen A und B)

F√ºr jede Dom√§ne-A- und -B-Antwort bewerten wir faktische Genauigkeit anhand der vorregistrierten Korrektantwortcheckliste. Jedes Checklistenelement ist bin√§r (vorhanden und korrekt, oder abwesend/falsch). Der faktische Genauigkeitsscore ist der Anteil erf√ºllter Checklistenelemente.

## 3.5 Synthesestrategien

Wir evaluieren drei Synthesestrategien:

**S1 - Mehrheitsvotum (Behauptungsebene):** Faktische Behauptungen werden aus allen Antworten extrahiert. Eine Behauptung wird in die Synthese aufgenommen, wenn sie in Antworten von mindestens 60 % der Modelle (sieben von zw√∂lf) erscheint. Minderheitsbehauptungen (unterhalb der Schwelle) werden mit einem [MINORITY]-Marker angeh√§ngt.

**S2 - Semantischer Zentroid:** Die Antwort, deren Embedding dem Mittelwert aller Antwort-Embeddings am n√§chsten liegt, wird als Synthesebasis ausgew√§hlt. Dies erfasst die "repr√§sentativste" einzelne Antwort. Es wird kein neuer Inhalt hinzugef√ºgt.

**S3 - LLM-as-Judge:** Alle zw√∂lf anonymisierten Antworten werden einer sechsten Modellinstanz (M2, claude-opus-4-6) mit der Anweisung pr√§sentiert, eine einzige autoritative Synthese zu erstellen, wobei Minderheitsbehauptungen ([MINORITY]) und Widerspr√ºche ([DISPUTED]) markiert werden. Das Richtermodell erh√§lt keine Informationen dar√ºber, welches Modell welche Antwort produziert hat.

Synthesequalit√§t wird durch Messung der faktischen Genauigkeit des resultierenden Texts gegen die vorregistrierten Korrektantworten (Dom√§nen A und B) und durch Experten-Rubrik-Scoring (Dom√§ne C) evaluiert.

## 3.6 Hypothesen

Wir testen drei vorregistrierte Hypothesen:

**H1 (Konvergenz in faktischen Dom√§nen):** Die mittlere paarweise semantische √Ñhnlichkeit f√ºr Dom√§ne-A- und -B-Prompts wird 0.75 (BERTScore F1) √ºbersteigen.

**H2 (Divergenz signalisiert Fehler):** Unter Dom√§ne-A- und -B-Antworten werden Ausrei√üermodelle (DBSCAN-Label -1) signifikant niedrigere faktische Genauigkeitsscores aufweisen als Nicht-Ausrei√üermodelle (einseitiger t-Test, Alpha=0.05).

**H3 (Dom√§neneffekt auf Konvergenz):** Die mittlere paarweise √Ñhnlichkeit f√ºr Dom√§ne C wird signifikant niedriger sein als f√ºr Dom√§nen A und B (einfaktorielle ANOVA, Post-hoc Tukey HSD, Alpha=0.05).

## 3.7 Experimentelles Setup

Alle Modelll√§ufe wurden √ºber das OpenClaw-Gateway ausgef√ºhrt, das unabh√§ngig zu den APIs jedes Anbieters routet. Jeder Prompt l√§uft in einer separaten isolierten Session ohne gemeinsamen Zustand. Laufausgaben werden als strukturiertes JSON mit Prompt-ID, Modell-ID, Antworttext, Token-Anzahl und Latenz gespeichert. Der vollst√§ndige Datensatz von 540 L√§ufen (45 Prompts x 5 Modelle) wird zusammen mit diesem Beitrag ver√∂ffentlicht.
