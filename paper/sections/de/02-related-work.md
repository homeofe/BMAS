# üìö 2. Verwandte Arbeiten

BMAS baut auf strukturierten Expertenkonsens-Methoden, Multi-Sample- und Multi-Modell-LLM-Techniken, automatisierten Evaluationsmetriken und dichtebasiertem Clustering auf. Dieser Abschnitt bespricht jeden Bereich und erl√§utert die Beziehung von BMAS zu fr√ºheren Arbeiten.

## 2.1 Die Delphi-Methode

Dalkey und Helmer (1963) entwickelten die Delphi-Methode bei der RAND Corporation als strukturierten Ansatz zur Expertenprognose. Im urspr√ºnglichen Protokoll gaben Expertengremien unabh√§ngig voneinander Sch√§tzungen ab, ohne die Antworten der anderen zu kennen, und ein Moderator aggregierte die Ergebnisse √ºber mehrere iterative Runden. Die zentrale St√§rke der Methode besteht darin, dass Isolation Verankerungseffekte und Gruppendenken verhindert und echte Meinungsverschiedenheiten sichtbar macht, bevor Konsens angestrebt wird. BMAS √ºbernimmt dieses Isolationsprinzip direkt: Jedes LLM antwortet auf Prompts, ohne die Ausgaben anderer Modelle zu kennen, sodass Konvergenz, wenn sie auftritt, unabh√§ngiges Denken widerspiegelt und keine Imitation.

## 2.2 Self-Consistency

Wang et al. (2022) schlugen Self-Consistency als Dekodierungsstrategie vor, die mehrere Reasoning-Ketten aus einem einzelnen Sprachmodell sampelt und die endg√ºltige Antwort per Mehrheitsvoting bestimmt. Diese Methode zeigte signifikante Verbesserungen bei arithmetischen und Common-Sense-Reasoning-Benchmarks durch die Ausnutzung der Intuition, dass korrekte Reasoning-Pfade wahrscheinlicher zur selben Antwort konvergieren als falsche. Da jedoch alle Reasoning-Ketten aus demselben Modell stammen, erfasst Self-Consistency nur die intramodale Dekodierungsvarianz, nicht die tieferen Unterschiede in Trainingsdaten, Architektur und Ausrichtung, die verschiedene Anbieter unterscheiden. BMAS erweitert die Konvergenz-als-Qualit√§tssignal-Intuition auf den anbieter√ºbergreifenden Rahmen, in dem √úbereinstimmung zwischen unabh√§ngig trainierten Modellen ein st√§rkeres Prior f√ºr Korrektheit darstellt.

## 2.3 Mixture of Agents

Wang et al. (2024) f√ºhrten das Mixture-of-Agents (MoA)-Framework ein, in dem mehrere LLMs an iterativen Aggregationsrunden teilnehmen, bei denen jedes Modell die Ausgaben der anderen beobachten und verfeinern kann. MoA demonstrierte, dass kollaborative Verbesserung √ºber Modelle hinweg die Leistung auf Benchmarks wie AlpacaEval und MT-Bench steigerte und Scores erzielte, die mit Frontier-Modellen vergleichbar sind. Der entscheidende Unterschied zu BMAS besteht darin, dass MoA nicht blind ist: Modelle in sp√§teren Runden sehen fr√ºhere Ausgaben, was das Risiko der Fehlerfortpflanzung birgt - eine selbstsichere Halluzination in einer fr√ºhen Runde kann nachfolgende Modelle beeinflussen und legitimen Widerspruch unterdr√ºcken. BMAS vermeidet dies bewusst durch strikte Isolation w√§hrend der Antwortphase und verschiebt jede modell√ºbergreifende Interaktion auf eine separate Synthesephase.

## 2.4 LLM-as-Judge

Zheng et al. (2023) untersuchten den Einsatz gro√üer Sprachmodelle als Evaluatoren anderer Modellausgaben und f√ºhrten die MT-Bench- und Chatbot-Arena-Benchmarks ein. Ihre Arbeit zeigte, dass starke LLMs als skalierbare Proxys f√ºr menschliche Evaluation dienen k√∂nnen und hohe √úbereinstimmung mit Expertengutachtern bei paarweisen Pr√§ferenzurteilen erzielten. In BMAS ist die Richterrolle dagegen auf eine von drei Synthesestrategien beschr√§nkt (S3): Ein sechstes Modell synthetisiert die zw√∂lf blinden Antworten zu einer einzigen Ausgabe, aber Korrektheit wird letztlich anhand vorregistrierter Korrektantworten gemessen, nicht anhand der Pr√§ferenzen des Richters.

## 2.5 BERTScore

Zhang et al. (2020) schlugen BERTScore vor, eine automatische Evaluationsmetrik, die token-level-√Ñhnlichkeit zwischen Kandidaten- und Referenztexten mithilfe kontextueller Embeddings von vortrainierten Transformermodellen berechnet. Im Gegensatz zu N-Gramm-√úberlappungsmetriken wie BLEU oder ROUGE erfasst BERTScore semantische √Ñquivalenz √ºber verschiedene Oberfl√§chenformen hinweg und ist robust gegen√ºber Paraphrasierungen. BMAS √ºbernimmt BERTScore F1 als prim√§re paarweise √Ñhnlichkeitsmetrik zur Messung modell√ºbergreifender Konvergenz, erg√§nzt durch Kosinus-√Ñhnlichkeit auf Satz-Embedding-Ebene f√ºr rechnerische Effizienz.

## 2.6 Constitutional AI

Bai et al. (2022) f√ºhrten Constitutional AI (CAI) bei Anthropic ein, eine Trainingsmethodik, bei der ein Modell seine eigenen Ausgaben gem√§√ü einem Satz von Prinzipien (einer "Verfassung") kritisiert und √ºberarbeitet, bevor Reinforcement Learning aus menschlichem Feedback eingesetzt wird. CAI zeigte, dass Selbstkritik sch√§dliche Ausgaben reduzieren kann, w√§hrend N√ºtzlichkeit erhalten bleibt. Dies stellt einen Einzelmodell-Ansatz zur Qualit√§tsverbesserung durch iterative Verbesserung dar. BMAS kann als Erweiterung dieser Kritik-und-√úberarbeitung-Intuition vom Einzelmodell-Loop auf einen Multi-Modell-, Multi-Anbieter-Rahmen betrachtet werden: Statt eines Modells, das sich selbst bewertet, dienen mehrere unabh√§ngig trainierte Modelle durch das Divergenzsignal als implizite Kritiker f√ºreinander.

## 2.7 DBSCAN

Ester et al. (1996) schlugen DBSCAN (Density-Based Spatial Clustering of Applications with Noise) vor, einen Clustering-Algorithmus, der Datenpunkte auf Basis von Dichtekonnektivit√§t gruppiert und Punkte in Niedrigdichtebereichen als Rauschen oder Ausrei√üer identifiziert. Im Gegensatz zu k-Means erfordert DBSCAN keine a-priori-Festlegung der Clusteranzahl und verarbeitet nat√ºrlich unregelm√§√üig geformte Cluster. BMAS setzt DBSCAN auf dem Embedding-Raum von Modellantworten ein, um Ausrei√üerausgaben zu erkennen: In faktischen Dom√§nen wird eine Ausrei√üerantwort - eine, die au√üerhalb der Epsilon-Nachbarschaft des Konsensclusters liegt - als potenzielle Halluzination behandelt, w√§hrend Ausrei√üer in strategischen Dom√§nen legitime Minderheitsansichten darstellen k√∂nnen.

## 2.8 Positionierung

BMAS ist nach unserem Kenntnisstand das erste Framework, das vier Eigenschaften kombiniert, die in keinem einzelnen fr√ºheren Ansatz zusammen vorhanden sind. Erstens erzwingt es anbieter√ºbergreifende Blind-Isolation: Im Gegensatz zu Self-Consistency, das aus einem Modell sampelt, sammelt BMAS Antworten von unabh√§ngig trainierten Modellen verschiedener Anbieter und erfasst Variation in Trainingsdaten, Architektur und Ausrichtung statt nur Dekodierungsstochastizit√§t. Zweitens f√ºhrt es dom√§nenstratifizierte Analyse ein und unterteilt Prompts in faktische, regulatorische und strategische Strata, um zu testen, ob Konvergenzmuster dom√§nenabh√§ngig sind - eine Dimension, die von MoA oder LLM-as-Judge-Evaluationen nicht untersucht wurde. Drittens behandelt es Divergenz als Anomaliesignal statt als Koordinationsversagen: Hohe modell√ºbergreifende Uneinigkeit in gut definierten Dom√§nen wird als Pr√§diktor f√ºr faktische Fehler hypothetisiert und verwandelt Uneinigkeit von einer St√∂rgr√∂√üe in ein Diagnosewerkzeug. Viertens bietet es einen kontrollierten Vergleich von Synthesestrategien (Mehrheitsvotum, semantischer Zentroid und LLM-as-Judge), evaluiert anhand vorregistrierter Korrektantworten, und liefert empirische Orientierung zur bestm√∂glichen Aggregation unabh√§ngiger Modellausgaben in der Praxis.
