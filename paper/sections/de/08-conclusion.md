# üéØ 8. Fazit

Dieser Beitrag stellte **Blind Multi-Agent Synthesis (BMAS)** vor, eine Methodik zur Erhebung, zum Vergleich und zur Synthese von Antworten mehrerer gro√üer Sprachmodelle in strikter Isolation, und pr√§sentierte empirische Ergebnisse aus einem 540-L√§ufe-Experiment √ºber f√ºnf Frontier-LLMs und drei Dom√§nenstrata.

## üìã 8.1 Zusammenfassung der Beitr√§ge

Wir haben demonstriert:

1. **Konvergenz ist dom√§nenabh√§ngig und messbar.** √úber 45 Prompts zeigten die Modelle A und B (technische und regulatorische Dom√§nen) konsistent h√∂here modell√ºbergreifende semantische √Ñhnlichkeit als Dom√§ne C (strategische und mehrdeutige Prompts). [Genaue Werte in Abschnitt 4.]

2. **Divergenz signalisiert Fehler in faktischen Dom√§nen.** Als semantische Ausrei√üer durch DBSCAN-Clustering identifizierte Modelle wiesen niedrigere faktische Genauigkeit gegen vorregistrierte Korrektantworten auf als Nicht-Ausrei√üermodelle, was Hypothese H2 st√ºtzt. Dies liefert eine empirische Grundlage f√ºr die Verwendung von Divergenz als praktisches Qualit√§tstor in KI-gest√ºtzten Entscheidungssystemen.

3. **Synthesequalit√§t variiert nach Strategie und Dom√§ne.** LLM-as-Judge (S3)-Synthese erzielte die h√∂chste faktische Genauigkeit in den Dom√§nen A und B, w√§hrend Mehrheitsvotum (S1) die umfassendste Abdeckung lieferte. Semantischer Zentroid (S2) schnitt als pr√§gnante repr√§sentative Zusammenfassung am besten ab. Keine einzelne Strategie dominierte √ºber alle Prompttypen.

4. **Modellausf√ºhrlichkeit ist kein Qualit√§ts-Proxy.** Wir beobachteten erhebliche Variation in der Antwort-Token-Anzahl √ºber Modelle bei identischen Prompts (bis zu 6,5-faches Verh√§ltnis bei einigen Prompts), ohne konsistente Korrelation zwischen Antwortl√§nge und faktischer Genauigkeit. Gemini 2.5-pro war konsistent das ausf√ºhrlichste Modell; Sonar das pr√§gnanteste. Diese stilistischen Unterschiede sagen keine Konvergenz voraus.

## 8.2 Praktische Erkenntnisse

F√ºr Praktiker, die LLMs in regulierten oder hochrisikobehafteten Umgebungen einsetzen, schl√§gt BMAS eine praktische Architektur vor: Prompts gegen mehrere unabh√§ngige Modellanbieter ausf√ºhren, semantische Konvergenz messen und Antworten mit geringer Konfidenz (hoher Divergenz) zur menschlichen Pr√ºfung weiterleiten. Der Overhead ist durch den Zuverl√§ssigkeitsgewinn gerechtfertigt, insbesondere f√ºr Compliance-kritische Fragen, bei denen eine einzelne falsche Antwort rechtliche oder sicherheitsrelevante Konsequenzen hat.

Das in dieser Studie verwendete Vorregistrierungsprotokoll - Korrektantworten vor jeglichen Modelll√§ufen zu sperren - ist auf jede Multi-Modell-Evaluationsanstrengung √ºbertragbar und verhindert den Best√§tigungsfehler, der entstehen kann, wenn Evaluatoren die Antworten kennen, bevor sie die Metriken entwerfen.

## 8.3 Beziehung zu AAHP und failprompt

BMAS wurde im Kontext von AAHP (AI-to-AI Handoff Protocol), einem strukturierten Multi-Agenten-Orchestrierungsframework f√ºr Produktions-KI-Pipelines, und failprompt, einem CLI-Tool zur Validierung von KI-Antworten in CI/CD-Umgebungen, entwickelt. Zusammen bilden diese drei Projekte ein integriertes Toolkit f√ºr verantwortungsvolles Multi-Modell-KI-Deployment: AAHP liefert die Orchestrierungsschicht, failprompt das CI-Tor, und BMAS die empirische Grundlage f√ºr das Verst√§ndnis, wann und warum Multi-Modell-Konsens zuverl√§ssiger ist als eine einzelne Modellausgabe.

Alle Code-, Prompt-, vorregistrierten Korrektantwort- und Experimentergebnisse werden als offene Datens√§tze ver√∂ffentlicht, um die Replikation und Erweiterung dieser Arbeit zu unterst√ºtzen.

---

*Der BMAS-Datensatz, Runner, die Metrik-Pipeline und der Synthesecode sind verf√ºgbar unter: https://github.com/homeofe/BMAS*
