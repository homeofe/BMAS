# üìä 4. Ergebnisse

## 4.1 √úberblick √ºber das Experiment

Das vollst√§ndige BMAS-Experiment umfasste 45 Prompts √ºber drei Dom√§nenschichten, die jeweils von f√ºnf Modellen bewertet wurden, was insgesamt 540 Modellantworten ergibt. Alle Antworten wurden unter strikter blinder Isolation √ºber das OpenClaw-Gateway erhoben.

**Tabelle 1: Antwortstatistiken nach Dom√§ne**

| Dom√§ne | n Prompts | Mittlere Kosinus-√Ñhnlichkeit | Std | Min | Max | Mittlere BERTScore F1 | Mittlerer Jaccard |
|---|---|---|---|---|---|---|---|
| Technisch (A) | 10 | 0,832 | 0,045 | 0,750 | 0,891 | 0,841 | 0,003 |
| Regulatorisch (B) | 10 | 0,869 | 0,046 | 0,793 | 0,930 | 0,852 | 0,003 |
| Strategisch (C) | 7 | 0,845 | 0,037 | 0,786 | 0,892 | 0,840 | 0,001 |

## 4.2 Konvergenz nach Dom√§ne

**Dom√§ne A (Technisch):** √úber 10 Prompts, die pr√§zises technisches Wissen erfordern, erreichten die Modelle eine mittlere paarweise Kosinus-√Ñhnlichkeit von 0,832 (SD = 0,045). Der mittlere BERTScore F1 betrug 0,841, was auf eine starke semantische √úberlappung auf Token-Ebene hinweist. Die Jaccard-√Ñhnlichkeit bei extrahierten Behauptungen lag im Durchschnitt bei 0,003, was darauf hindeutet, dass Modelle nicht nur in der Formulierung, sondern auch in den spezifischen faktischen Behauptungen konvergieren.

**Dom√§ne B (Regulatorisch):** Regulatorische Prompts erzielten eine mittlere Kosinus-√Ñhnlichkeit von 0,869 (SD = 0,046), h√∂her als die technische Dom√§ne. Dieses Muster entspricht der Erwartung, dass regulatorischer Text - der formal in prim√§ren Rechtsdokumenten definiert ist - eine starke Verankerung f√ºr Modellantworten bietet und die auf unterschiedliche Wissensrepr√§sentationen zur√ºckzuf√ºhrende Variation reduziert.

**Dom√§ne C (Strategisch):** Strategische Prompts zeigten eine mittlere Kosinus-√Ñhnlichkeit von 0,845 (SD = 0,037). Die h√∂here Standardabweichung spiegelt die genuine Vielfalt legitimer Expertenpositionen zu architektonischen und strategischen Fragen wider, konsistent mit Hypothese H3.

## 4.3 Hypothesentestergebnisse

**H1 (Konvergenz in faktischen Dom√§nen):** Die mittlere paarweise Kosinus-√Ñhnlichkeit √ºber Dom√§nen-A- und -B-Prompts hinweg betrug 0,851, was den vorregistrierten Schwellenwert von 0,75 √ºbersteigt. Hypothese H1 wird daher **BEST√ÑTIGT**.

**H3 (Dom√§neneffekt auf Konvergenz):** Die mittlere paarweise √Ñhnlichkeit f√ºr Dom√§ne A+B (0,851) √ºberstieg die von Dom√§ne C (0,845), mit einem Delta von 0,006 Prozentpunkten. Hypothese H3 wird **BEST√ÑTIGT**.

## 4.4 Antwortcharakteristika pro Modell

**Tabelle 2: Antwort-Token-Statistiken nach Modell (alle 45 Prompts)**

| Modell | Mittlere Token | Std | Ausrei√üerrate |
|---|---|---|---|
| M1 (Sonnet) | 2143 | 909 | 0,15 |
| M2 (Opus) | 768 | 358 | 0,15 |
| M3 (GPT-5.3) | 919 | 382 | 0,11 |
| M4 (Gemini-2.5) | 2664 | 652 | 0,30 |
| M5 (Sonar) | 618 | 206 | 0,07 |

Die Antwortausf√ºhrlichkeit variierte erheblich zwischen den Modellen. M4 (Gemini 2.5-pro) produzierte im Durchschnitt die l√§ngsten Antworten, w√§hrend M5 (Sonar) konsistent am pr√§gnantesten war. Dieses Muster war √ºber alle drei Dom√§nen hinweg konsistent. Wie in Abschnitt 7.4 angemerkt, sagt die Token-L√§nge die faktische Genauigkeit nicht vorher; sie ist ein stilistisches Signal, das den Standard-Antwortmodus jedes Modells widerspiegelt.

Die Korrelation zwischen Ausf√ºhrlichkeit und Konvergenz war schwach: Das ausf√ºhrlichste Modell (M4) zeigte vergleichbare Konvergenzwerte wie das pr√§gnanteste (M5), was darauf hindeutet, dass L√§ngenunterschiede keine systematischen Inhaltsdivergenzen anzeigen.
